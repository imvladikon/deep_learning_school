#!/usr/bin/env python
# coding: utf-8

# In[1]:


get_ipython().run_cell_magic('capture', '', '!pip install -q transformers datasets tokenizers')


# # –î–æ–º–∞—à–Ω–µ–µ –∑–∞–¥–∞–Ω–∏–µ. Transformers.
# 
# –ü—Ä–∏–≤–µ—Ç! –≠—Ç–æ –æ—á–µ—Ä–µ–¥–Ω–æ–µ –¥–æ–º–∞—à–Ω–µ–µ –∑–∞–¥–∞–Ω–∏–µ, –Ω–æ —Ç–µ–ø–µ—Ä—å —Ç—ã –ø–æ–∑–Ω–∞–∫–æ–º–∏—à—å—Å—è —Å –º–æ–¥–µ–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –∏ —Å –±–∏–±–ª–∏–æ—Ç–µ–∫–æ–π `HuggingFaceü§ó`. –í —ç—Ç–æ–º –∑–∞–¥–∞–Ω–∏–∏ –±—É–¥–µ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–æ —Ä–µ—à–∏—Ç—å —Å –ø–æ–º–æ—â—å—é –º–æ–¥–µ–ª–∏ `GPT2` –ø—Ä–æ—Å—Ç—É—é –∑–∞–¥–∞—á—É (–∞–Ω–∞–ª–∏–∑ —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç–∞) –∏ —Å–¥–µ–ª–∞—Ç—å –Ω–µ–±–æ–ª—å—à–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∫–∞—Ä—Ç –≤–Ω–∏–º–∞–Ω–∏—è. –ü—Ä–∏—Å—Ç—É–ø–∏–º!

# In[2]:


import numpy as np
import matplotlib.pyplot as plt
from matplotlib import ticker

import torch
import torch.nn as nn

from transformers import GPT2ForSequenceClassification, GPT2TokenizerFast, GPT2Config
from datasets import load_dataset


# In[3]:


device = "cuda" if torch.cuda.is_available else "cpu"


# –î–∞—Ç–∞—Å–µ—Ç, –∫–æ—Ç–æ—Ä—ã–π –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–µ–≥–æ–¥–Ω—è ‚Äì —Ç–µ–∫—Å—Ç—ã –∏–∑ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —Ç–≤–∏—Ç—Ç–µ—Ä–∞. –û–Ω–∏ —É–∂–µ –ø–æ—á–∏—â–µ–Ω–Ω—ã –æ—Ç –Ω–∏–∫–Ω–µ–π–º–æ–≤, –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏ –ø—Ä–æ—á–µ–≥–æ. 

# In[4]:


emotion_dataset = load_dataset("emotion")


# –ü–æ—Å–º–æ—Ç—Ä–∏, –∏–∑ —á–µ–≥–æ —Å–æ—Å—Ç–æ–∏—Ç `emotion_dataset`:

# In[6]:


emotion_dataset


# In[7]:


emotion_dataset["train"]


# In[8]:


emotion_dataset["train"]["text"][0]


# In[9]:


emotion_dataset["train"]["label"][0]


# In[10]:


len(emotion_dataset["train"])


# –î–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ —Ç–µ–∫—Å—Ç–∞ –≤ —Ç–æ–∫–µ–Ω—ã –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–π BPE-—Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä.

# In[6]:


tokenizer = GPT2TokenizerFast.from_pretrained("distilgpt2")
tokenizer.pad_token = tokenizer.eos_token # –£ gpt2 –Ω–µ—Ç pad —Ç–æ–∫–µ–Ω–æ–≤. –í–º–µ—Å—Ç–æ –Ω–∏—Ö –≤–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è —Ç–æ–∫–µ–Ω–∞–º–∏ –∫–æ–Ω—Ü–∞ —Ç–µ–∫—Å—Ç–∞.


# –ü–æ–¥–≥–æ—Ç–æ–≤—å –∫–ª–∞—Å—Å, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç, —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä –∏ –∏–º—è –∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–π —á–∞—Å—Ç–∏ (`train`, `validation`, `test`). –ò—Å–ø–æ–ª—å–∑—É–π –µ–≥–æ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.
# 
# P.S. –ü–æ—Å–º–æ—Ç—Ä–∏, –∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä ([docs](https://huggingface.co/transformers/main_classes/tokenizer.html)) –∏ –ø–æ–¥—É–º–∞–π, –∫–∞–∫ –µ–≥–æ –Ω–∞–¥–æ –¥–æ–±–∞–≤–∏—Ç—å –≤ –¥–∞—Ç–∞—Å–µ—Ç.

# –ù–µ–º–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤, –∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä. –≠—Ç–æ –ø–æ–º–æ–∂–µ—Ç —Å –Ω–∞–ø–∏—Å–∞–Ω–∏–µ–º –¥–∞—Ç–∞—Å–µ—Ç–∞.

# In[12]:


tokenizer.tokenize(emotion_dataset["train"]["text"][0])


# In[13]:


tokenizer.encode(emotion_dataset["train"]["text"][0])


# In[14]:


tokenizer.encode_plus(emotion_dataset["train"]["text"][0])


# In[15]:


tokenizer.encode_plus(emotion_dataset["train"]["text"][0], return_tensors="pt")


# In[16]:


tokenizer.encode_plus(
    emotion_dataset["train"]["text"][0], 
    max_length=128, # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞
    padding="max_length", # –Ω–∞–¥–æ –ª–∏ –¥–æ–±–∞–≤–ª—è—Ç—å –ø–∞–¥–¥–∏–Ω–≥ –≤ –∫–æ–Ω—Ü–µ?
    return_tensors="pt", # –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç pytorch —Ç–µ–Ω–∑–æ—Ä—ã
)


# In[ ]:


# –ï—Å–ª–∏ –Ω–∞–¥–æ, –ø–æ–ø—Ä–∞–∫—Ç–∏–∫—É–π—Å—è —Ä–∞–±–æ—Ç–∞—Ç—å —Å —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–æ–º –∑–¥–µ—Å—å


# In[10]:


class TweetDataset(torch.utils.data.Dataset):
    def __init__(self, part, dataset=emotion_dataset, tokenizer=tokenizer, max_length=128):
        self.part = part
        self.dataset = dataset
        self.tokenizer = tokenizer
        self.max_length = max_length
        
        self.labels = np.unique(dataset[part]["label"])
        self.label2num = {l: num for num, l in enumerate(self.labels)}
        
    def __getitem__(self, idx):
        """
        Return dict with tokens, attention_mask and label
        """
        text = self.dataset[self.part]['text'][idx]
        label = self.dataset[self.part]['label'][idx]
        
        tokenizer_output = self.tokenizer.encode_plus(text,max_length=128, padding="max_length",return_tensors="pt")
        target = self.label2num[label]
        return {
            "input_ids": tokenizer_output["input_ids"], 
            "mask": tokenizer_output['attention_mask'],
            "target": target
        }
        
    def __len__(self):
        """
        Return length of dataset
        """
        return len(self.dataset[self.part])


# –°–æ–∑–¥–∞–π `train`, `validation` –∏ `test` —á–∞—Å—Ç–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞. –ó–∞–≥—Ä—É–∑–∏ –∏—Ö –≤ `DataLoaders`.

# In[11]:


train_dataset = TweetDataset("train")
valid_dataset = TweetDataset("validation")
test_dataset = TweetDataset("test")


# In[12]:


batch_size = 64

train_loader = torch.utils.data.DataLoader(
    dataset=train_dataset,batch_size = batch_size
)
valid_loader = torch.utils.data.DataLoader(
    dataset=valid_dataset,batch_size = batch_size
)
test_loader = torch.utils.data.DataLoader(
    dataset=test_dataset,batch_size = batch_size
)


# ## –ù–∞—á–Ω–µ–º —Å –Ω—É–ª—è.
# 
# –ü–æ–ø—Ä–æ–±—É–µ–º –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä —Å –Ω—É–ª—è —Ä–µ—à–∞—Ç—å –¥–∞–Ω–Ω—É—é –∑–∞–¥–∞—á—É.

# In[25]:


config = GPT2Config.from_pretrained(
    "distilgpt2", # distilgpt2 ‚Äì¬†—É–º–µ–Ω—å—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –º–æ–¥–µ–ª–∏ gpt2
    output_attentions=True,
    pad_token_id=tokenizer.eos_token_id,
    num_labels=8
)
model_0 = GPT2ForSequenceClassification(config=config).to(device) # GPT2 –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞


# –ü–æ–¥–≥–æ—Ç–æ–≤—å –æ–ø—Ç–∏–º–∞–π–∑–µ—Ä –∏ –∫—Ä–∏—Ç–µ—Ä–∏–π:

# In[26]:


lr = 1e-5 # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º—ã–π learning rate. –û–Ω –º–æ–∂–µ—Ç –±—ã—Ç—å –±–æ–ª—å—à–µ –∏–ª–∏ –º–µ–Ω—å—à–µ :)

optimizer = torch.optim.Adam(model_0.parameters(),lr=lr)
criterion = nn.CrossEntropyLoss()
# scheduler = get_cosine_schedule_with_warmup(optimizer, 0, 10)


# –ü–æ—Å–º–æ—Ç—Ä–∏, —á—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–æ–¥–µ–ª—å ([docs](https://huggingface.co/transformers/model_doc/gpt2.html#gpt2forsequenceclassification)), –µ—Å–ª–∏ –≤ –Ω–µ—ë –ø–æ–¥–∞—Ç—å –¥–∞–Ω–Ω—ã–µ:

# In[22]:


d_0 = train_dataset[0] 
tokens = d_0['input_ids'].to(device) # –ü–æ–ª—É—á–∏ —Ç–æ–∫–µ–Ω—ã –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞
mask = d_0['mask'].to(device) # –ü–æ–ª—É—á–∏ –º–∞—Å–∫—É –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞

model_0(tokens,attention_mask=mask) # –ü–æ—Å–º–æ—Ç—Ä–∏ –Ω–∞ –∞—É—Ç–ø—É—Ç


# –û–±—É—á–∏ –º–æ–¥–µ–ª—å —Å –ø–æ–º–æ—â—å—é `train_dataset`, –ø—Ä–æ–≤–µ—Ä—è–π –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–æ —Å –ø–æ–º–æ—â—å—é `valid_dataset` –∏ –ø–æ–ª—É—á–∏ —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ —Å –ø–æ–º–æ—â—å—é `test_dataset`.

# In[27]:


from tqdm.notebook import tqdm
import gc

num_epochs = 5 # 10 –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é —Å—Ç–æ–∏—Ç –≤ –Ω–æ—É—Ç–±—É–∫–µ, –Ω–æ –¥—É–º–∞—é –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –º–µ–Ω—å—à–µ, –Ω–∞ —Ç–∞–∫–∏—Ö –±–æ–ª—å—à–∏—Ö LM –æ–Ω–æ –Ω–∞—á–∏–Ω–∞–µ—Ç –æ–≤–µ—Ä—Ñ–∏—Ç–∏—Ç—Å—è (–ø–æ—Å–ª–µ –ø—Ä–æ–≥–æ–Ω–∞ 10 —ç–ø–æ—Ö –±—ã–ª–æ –≤–∏–¥–Ω–æ)

def accuracy_(outputs, labels):
  proba = torch.softmax(outputs,dim=1)
  y_hat = proba.argmax(dim=1)
  return (y_hat==labels).float().mean()

# Train loop
for e in range(num_epochs):
    gc.collect()
    torch.cuda.empty_cache()
    model_0.train()
    train_loss, train_acc = 0, 0
    for batch in tqdm(train_loader):
        targets = batch['target'].to(device)
        input_ids = batch['input_ids'].squeeze(1).to(device)
        attention_mask  = batch['mask'].squeeze(1).to(device)
        optimizer.zero_grad()
        outputs = model_0(input_ids=input_ids,attention_mask=attention_mask,labels=targets)
        loss, outputs = outputs["loss"], outputs["logits"]
        train_loss += criterion(outputs,targets)
        # loss = criterion(outputs,targets) # GPT —É–∂–µ –ø–æ—Å—á–∏—Ç–∞–ª loss
        # print(torch.allclose(loss, criterion(outputs,targets))) -> True
        loss.backward()
        optimizer.step()
        # scheduler.step()
        train_acc += accuracy_(outputs,targets).item() 
    
    print(f"Train Loss: {train_loss / len(train_loader)},"
          f"Train Acc: {train_acc / len(train_loader)}")
    
    
        
    valid_loss = 0
    valid_acc = 0
    model_0.eval()
    with torch.no_grad():
        for batch in valid_loader:
            targets = batch['target'].to(device)
            input_ids = batch['input_ids'].squeeze(1).to(device)
            attention_mask  = batch['mask'].squeeze(1).to(device)
            outputs = model_0(input_ids=input_ids,attention_mask=attention_mask,labels=targets)
            loss, outputs = outputs["loss"], outputs["logits"]
            valid_loss += loss #criterion(outputs, targets)
            valid_acc += accuracy_(outputs,targets).item() 

    print(f"Valid Loss: {valid_loss / len(valid_loader)},"
          f"Valid Acc: {valid_acc / len(valid_loader)}")


# In[28]:


# Testing
test_acc = 0
model_0.eval()
with torch.no_grad():
    for batch in test_loader:
        targets = batch['target'].to(device)
        input_ids = batch['input_ids'].squeeze(1).to(device)
        attention_mask  = batch['mask'].squeeze(1).to(device)
        outputs = model_0(input_ids=input_ids,attention_mask=attention_mask,labels=targets)
        outputs, loss = outputs["logits"], outputs["loss"]
        test_acc += accuracy_(outputs,targets).item()
print(f"Test Acc: {test_acc / len(test_loader)}")


# –ü–æ—Å–ª–µ —Ç–æ–≥–æ, –∫–∞–∫ –ø–æ–ª—É—á–∏–ª –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –∑–∞–¥–∞—á–∏, –ø–æ—Å–º–æ—Ç—Ä–∏ –Ω–∞ –∫–∞—Ä—Ç—ã –≤–Ω–∏–º–∞–Ω–∏—è. –ù–∞—à–µ–ª –ª–∏ —á—Ç–æ-–Ω–∏–±—É–¥—å –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ–µ –≤ –Ω–∏—Ö?

# In[20]:


def get_attention_matrixes(model, tokenizer, text, device=device):
    inp = list(filter(lambda x: x != tokenizer.sep_token_id, tokenizer.encode(text)))
    inp = torch.tensor(inp, dtype=torch.long, device=device).unsqueeze(0)
    attn_tensors = model(inp)[-1]
    seq = [tokenizer.decode(x) for x in inp[0].tolist()]
    attn = []
    for i in range(len(attn_tensors)):
        attn_layer = []
        for j in range(attn_tensors[i].size(1)):
            attn_layer.append(attn_tensors[i][0, j].cpu().detach().numpy())
        attn.append(np.array(attn_layer))
    
    return np.array(attn)


# In[21]:


def show_attention(seq, attentions):
    # Set up figure with colorbar
    fig = plt.figure(figsize=(20,20))
    ax = fig.add_subplot(111)
    cax = ax.matshow(attentions)
    fig.colorbar(cax)

    # Set up axes
    ax.set_xticklabels(['']+seq, rotation=90, fontsize=16)
    ax.set_yticklabels(['']+seq, fontsize=16)

    # Show label at every tick
    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))
    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))

    plt.show()


# In[35]:


text = emotion_dataset['train']['text'][42] # –í—ã–±–µ—Ä–∏ —Ç–µ–∫—Å—Ç –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞
tokens = tokenizer.tokenize(text)


# In[39]:


attns = get_attention_matrixes(model_0, tokenizer, text)
show_attention(tokens, attns[-1][0])


# –∫–∞–∫ –≤–∏–¥–∏–º –∞—Ç—Ç–µ–Ω—à–Ω —Ä–∞–∑–º–∞–∑–∞–Ω –ø–æ —Å–ª–æ–≤–∞–º, –º–æ–¥–µ–ª—å –Ω–µ —É—Å–ø–µ–ª–∞ –µ—â–µ –∞–∫–∫—É–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–≤—è–∑—è—Ö –≤ —Å–ª–æ–≤–∞—Ö

# –ø–æ–ø—Ä–æ–±—É–µ–º –ø–æ–º–µ–Ω—è—Ç—å –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ –¥–æ–±–∞–≤–∏—Ç—å lr scheduler –∏–∑ –ø–∞–∫–µ—Ç–∞ transformers, –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –º–æ–¥–µ–ª—å —Å–æ–π–¥–µ—Ç—Å—è –∑–∞ –º–µ–Ω—å—à–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö —á–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∞—è

# In[42]:


from transformers import AdamW, get_linear_schedule_with_warmup


model_0 = GPT2ForSequenceClassification(config=config).to(device) # GPT2 –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞
criterion = nn.CrossEntropyLoss() # –≤ –ø—Ä–∏–Ω—Ü–∏–ø–µ –ª–∏—à–Ω–µ–µ, –≤–Ω—É—Ç—Ä–∏ GPT2ForSequenceClassification —É–∂–µ –æ–Ω –µ—Å—Ç—å
optimizer = AdamW(model_0.parameters(),
                  lr = 2e-5,
                  eps = 1e-8
                  )

num_epochs = 5
total_steps = len(train_loader) * num_epochs
scheduler = get_linear_schedule_with_warmup(optimizer, 
                                            num_warmup_steps = 0, # Default value in run_glue.py
                                            num_training_steps = total_steps)




for e in range(num_epochs):
    gc.collect()
    torch.cuda.empty_cache()
    model_0.train()
    train_loss, train_acc = 0, 0
    for batch in tqdm(train_loader):
        targets = batch['target'].to(device)
        input_ids = batch['input_ids'].squeeze(1).to(device)
        attention_mask  = batch['mask'].squeeze(1).to(device)
        optimizer.zero_grad()
        outputs = model_0(input_ids=input_ids,attention_mask=attention_mask,labels=targets)
        loss, outputs = outputs["loss"], outputs["logits"]
        train_loss += criterion(outputs,targets)
        loss.backward()
        optimizer.step()
        scheduler.step()
        train_acc += accuracy_(outputs,targets).item() 
    
    print(f"Train Loss: {train_loss / len(train_loader)},"
          f"Train Acc: {train_acc / len(train_loader)}")
    
    
    valid_loss = 0
    valid_acc = 0
    model_0.eval()
    with torch.no_grad():
        for batch in valid_loader:
            targets = batch['target'].to(device)
            input_ids = batch['input_ids'].squeeze(1).to(device)
            attention_mask  = batch['mask'].squeeze(1).to(device)
            outputs = model_0(input_ids=input_ids,attention_mask=attention_mask,labels=targets)
            loss, outputs = outputs["loss"], outputs["logits"]
            valid_loss += loss
            valid_acc += accuracy_(outputs,targets).item() 

    print(f"Valid Loss: {valid_loss / len(valid_loader)},"
          f"Valid Acc: {valid_acc / len(valid_loader)}")
    
test_acc = 0
model_0.eval()
with torch.no_grad():
    for batch in test_loader:
        targets = batch['target'].to(device)
        input_ids = batch['input_ids'].squeeze(1).to(device)
        attention_mask  = batch['mask'].squeeze(1).to(device)
        outputs = model_0(input_ids=input_ids,attention_mask=attention_mask,labels=targets)
        outputs, loss = outputs["logits"], outputs["loss"]
        test_acc += accuracy_(outputs,targets).item()
print(f"Test Acc: {test_acc / len(test_loader)}")


# –∫–∞–∫ –º—ã –∏ –æ–∂–∏–¥–∞–ª–∏, –ø–æ–ª—É—á–∏–ª–∏ —Ç–æ—Ç –∂–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –Ω–æ –ø—Ä–æ—Ü–µ—Å—Å —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –±–æ–ª–µ–µ –≥–ª–∞–¥–∫–∏–π (—ç–ø–æ—Ö –º–æ–∂–Ω–æ –±—ã–ª–æ –≤—ã–±—Ä–∞—Ç—å 4, –Ω–∞ 5 —É–∂–µ –æ–≤–µ—Ä—Ñ–∏—Ç–∏—Ç—Å—è)

# ## Fine-tuning
# 
# –¢–µ–ø–µ—Ä—å –¥—Ä—É–≥–æ–π –ø–æ–¥—Ö–æ–¥: –∑–∞–≥—Ä—É–∑–∏–º –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –æ–±—É—á–∞–ª–∞—Å—å —Ä–µ—à–∞—Ç—å –∑–∞–¥–∞—á—É Language Modeling. –ü–æ—Å–º–æ—Ç—Ä–∏–º, –ø–æ–ª—É—á–∏–º –ª–∏ –º—ã –ø—Ä–∏—Ä–æ—Å—Ç –≤ –∫–∞—á–µ—Å—Ç–≤–µ.

# In[7]:


model_1 = GPT2ForSequenceClassification.from_pretrained(
    "distilgpt2", 
    output_attentions=True,
    pad_token_id=tokenizer.eos_token_id,
    num_labels=8
).to(device)


# In[13]:


from transformers import AdamW, get_linear_schedule_with_warmup
# lr = 1e-5 # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º—ã–π learning rate. –û–Ω –º–æ–∂–µ—Ç –±—ã—Ç—å –±–æ–ª—å—à–µ –∏–ª–∏ –º–µ–Ω—å—à–µ :)

criterion = nn.CrossEntropyLoss() # –≤ –ø—Ä–∏–Ω—Ü–∏–ø–µ –ª–∏—à–Ω–µ–µ, –≤–Ω—É—Ç—Ä–∏ GPT2ForSequenceClassification —É–∂–µ –æ–Ω –µ—Å—Ç—å
optimizer = AdamW(model_1.parameters(),
                  lr = 2e-5,
                  eps = 1e-8
                  )

num_epochs = 10
total_steps = len(train_loader) * num_epochs
scheduler = get_linear_schedule_with_warmup(optimizer, 
                                            num_warmup_steps = 0, # Default value in run_glue.py
                                            num_training_steps = total_steps)


# –í—ã–≤–æ–¥ –º–æ–¥–µ–ª–∏ –Ω–∏—á–µ–º –Ω–µ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Å–ª—É—á–∞—è, –ø–æ—ç—Ç–æ–º—É —Å—Ä–∞–∑—É –ø—Ä–∏—Å—Ç—É–ø–∞–µ–º –∫ –æ–±—É—á–µ–Ω–∏—é:

# In[17]:


from tqdm.notebook import tqdm


# Train loop
for e in range(num_epochs):
    model_1.train()
    train_acc, train_loss = 0, 0
    for batch in tqdm(train_loader):
        targets = batch['target'].to(device)
        input_ids = batch['input_ids'].squeeze(1).to(device)
        attention_mask  = batch['mask'].squeeze(1).to(device)
        optimizer.zero_grad()
        outputs = model_1(input_ids=input_ids,attention_mask=attention_mask,labels=targets)
        loss, outputs = outputs["loss"], outputs["logits"]
        train_loss += criterion(outputs,targets)
        # loss = criterion(outputs,targets) # GPT —É–∂–µ –ø–æ—Å—á–∏—Ç–∞–ª loss
        loss.backward()
        optimizer.step()
        # scheduler.step()
        train_acc += accuracy_(outputs,targets).item() 

    print(f"Train Loss: {train_loss / len(train_loader)},"
          f"Train Acc: {train_acc / len(train_loader)}")
    
        
    valid_loss = 0
    valid_acc = 0
    model_1.eval()
    with torch.no_grad():
        for batch in valid_loader:
            targets = batch['target'].to(device)
            input_ids = batch['input_ids'].squeeze(1).to(device)
            attention_mask  = batch['mask'].squeeze(1).to(device)
            outputs = model_1(input_ids=input_ids,attention_mask=attention_mask,labels=targets)
            loss, outputs = outputs["loss"], outputs["logits"]
            valid_loss += loss #criterion(outputs, targets)
            valid_acc += accuracy_(outputs,targets).item() 


    print(f"Valid Loss: {valid_loss / len(valid_loader)},"
          f"Valid Acc: {valid_acc / len(valid_loader)}")


# In[18]:


test_acc = 0
model_1.eval()
with torch.no_grad():
    for batch in test_loader:
        targets = batch['target'].to(device)
        input_ids = batch['input_ids'].squeeze(1).to(device)
        attention_mask  = batch['mask'].squeeze(1).to(device)
        outputs = model_1(input_ids=input_ids,attention_mask=attention_mask,labels=targets)
        loss, outputs = outputs["loss"], outputs["logits"]
        test_acc += accuracy_(outputs,targets).item()

print(f"Test Acc: {test_acc / len(test_loader)}")


# –ï—Å—Ç—å –ª–∏ –ø—Ä–∏—Ä–æ—Å—Ç –∫–∞—á–µ—Å—Ç–≤–∞ –∏–ª–∏ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è?
# 
# –ü–æ—Å–º–æ—Ç—Ä–∏ –Ω–∞ –∫–∞—Ä—Ç—ã –≤–Ω–∏–º–∞–Ω–∏—è. –ï—Å—Ç—å –ª–∏ –æ—Ç–ª–∏—á–∏–µ –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Å–ª—É—á–∞—è?

# –¥–∞, –∫–æ–Ω–µ—á–Ω–æ, –ø—Ä–∏—Ä–æ—Å—Ç –∑–Ω–∞—á–∏—Ç–µ–ª–µ–Ω, –∫–∞—Ä—Ç—ã –≤–Ω–∏–º–∞–Ω–∏—è –¥—É–º–∞—é —Ç–∞–∫–∂–µ –∏–∑–º–µ–Ω–∏–ª–∏—Å—å

# In[19]:


text = emotion_dataset['train']['text'][42] # –í—ã–±–µ—Ä–∏ —Ç–µ–∫—Å—Ç –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞
tokens = tokenizer.tokenize(text)


# In[22]:


attns = get_attention_matrixes(model_1, tokenizer, text)
show_attention(tokens, attns[-1][0])


# –∫–∞—Ä—Ç—ã –≤–Ω–∏–º–∞–Ω–∏—è –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —ç—Ç–∞ –º–æ–¥–µ–ª—å —É–∂–µ –∏–º–µ–µ—Ç –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ "–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ" –æ —è–∑—ã–∫–µ –∏ —Å–≤—è–∑—ã–≤–∞–µ—Ç —Å–ª–æ–≤–∞ –Ω–∞—Ö–æ–¥—è—â–∏–µ—Å—è –Ω–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —Ä—è–¥–æ–º

# ## –û—Ç—á–µ—Ç
# 
# –ü–æ–∫–∞–∂–∏ –∑–¥–µ—Å—å, —á—Ç–æ —Ç—ã –≤—ã–ø–æ–ª–Ω–∏–ª –ø–æ —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ. –û—Ç–≤–µ—Ç—å –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–æ–ø—Ä–æ—Å–æ–≤:
# - –ö–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ –æ–∫–∞–∑–∞–ª—Å—è –ª—É—á—à–µ? 
# - –ù–∞ –∫–∞–∫–∏–µ —Å–ª–æ–≤–∞ –º–æ–¥–µ–ª—å –±–æ–ª—å—à–µ–≥–æ –≤—Å–µ–≥–æ –æ–±—Ä–∞—â–∞–ª–∞ –≤–Ω–∏–º–∞–Ω–∏–µ?
# - –ù–∞ –∫–∞–∫–∏—Ö —Å–ª–æ—è—Ö/–≥–æ–ª–æ–≤–∞—Ö –º–æ–¥–µ–ª—å –æ–±—Ä–∞—â–∞–ª–∞ –≤–Ω–∏–º–∞–Ω–∏–µ?
# 
# < —Ç–≤–æ–π –æ—Ç—á–µ—Ç/–æ—Ç–≤–µ—Ç—ã >

# –ú–æ–¥–µ–ª—å —Å –ø—Ä–µ–¥—ä–æ–±—É—á–µ–Ω–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏ –±–µ–∑—É—Å–ª–æ–≤–Ω–æ –ª—É—á—à–µ –≤ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ —Å –æ–±—ã—á–Ω–æ–π –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º –∫–∞—á–µ—Å—Ç–≤–∞.

# <div id="markdownResult" style="background-color: rgb(255, 255, 255); margin: auto; width: 290px;"><table><tbody><tr><th align="center"></th><th align="center"><b>vanilla model</b></th><th align="center"><b>pretrained model</b></th></tr><tr><td align="center">Test Acc:</td><td align="center">0.79</td><td align="center">0.92</td></tr>
# </tbody></table></div>

# –æ–±—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å –±–æ–ª—å—à–µ –æ–±—Ä–∞—â–∞–µ—Ç –≤–Ω–∏–º–∞–Ω–∏–µ –≤—Å–µ –∂–µ –Ω–∞ —Å–æ—Å–µ–¥–Ω–∏–µ —Å–ª–æ–≤–∞, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è —É–∂–µ –∏–º–µ–µ—Ç –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ "–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ" –æ —è–∑—ã–∫–µ –∏ —Å–≤—è–∑—ã–≤–∞–µ—Ç —Å–ª–æ–≤–∞ –Ω–∞—Ö–æ–¥—è—â–∏–µ—Å—è –Ω–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —Ä—è–¥–æ–º
